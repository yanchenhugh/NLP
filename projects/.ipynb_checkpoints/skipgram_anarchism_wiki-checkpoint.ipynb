{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9a1f1e-cf46-4992-8817-0f3696020fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n",
      "Total words in text: 16680599\n",
      "Unique words: 63641\n",
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580]\n",
      "[5233, 10571, 27349, 854, 15067, 58112, 10712, 2731, 371, 539, 2757, 7088, 247, 5233, 44611, 792, 5233, 602, 8983, 4147, 4186, 153, 5233, 447, 1818, 4860, 6753, 7573, 1774, 566]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# read in the extracted text file      \n",
    "with open('../data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get list of words\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])\n",
    "\n",
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words\n",
    "\n",
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "#print(list(word_counts.items())[0:9])  # dictionary of int_words, how many times they appear\n",
    "\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "def discard_prob(threshold, frac):\n",
    "    return 1 - np.sqrt(threshold/frac)\n",
    "\n",
    "train_words = [idx for idx in int_words if random.random() > discard_prob(threshold, word_counts[idx]/len(int_words)) ]\n",
    "print(train_words[:30])\n",
    "\n",
    "# Since the more distant words are usually less related to the current word than those close to it, \n",
    "# we give less weight to the distant words by sampling less from those words in our training examples.\n",
    "# If we choose ð¶=5, for each training word we will select randomly a number ð‘… in range  [1:ð¶], \n",
    "# and then use ð‘… words from history and ð‘… words from the future of the current word as correct labels.\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    # implement this function\n",
    "    R = np.ceil(random.random()/0.2).astype(np.int64)\n",
    "    past_idx = list(range(np.max([idx-R, 0]), idx))\n",
    "    future_idx = list(range(idx+1, np.min([idx+R+1, len(words)])))\n",
    "    \n",
    "    return [words[idx] for idx in past_idx + future_idx]\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "\n",
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369f637c-7e68-4c94-a746-158d86954f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Now define this SkipGram model\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        # complete this SkipGram model\n",
    "        self.n_vocabulary = n_vocab\n",
    "        self.n_embedding = n_embed\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_embed, n_vocab)\n",
    "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the forward behavior\n",
    "        x_embed = self.in_embed(x)\n",
    "        x_out = self.fc(x_embed)\n",
    "        x = self.LogSoftmax(x_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define the skipgram model with negative sampling\n",
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        # define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        # Initialize both embedding tables with uniform distribution\n",
    "        self.in_embed.weight.data.uniform_(-1, 1)\n",
    "        self.out_embed.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        # return input vector embeddings\n",
    "\n",
    "        return self.in_embed(input_words)\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        # return output vector embeddings\n",
    "\n",
    "        return self.out_embed(output_words)\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed) \"\"\"\n",
    "        if self.noise_dist is None:\n",
    "            # Sample words uniformly\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # Sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist,\n",
    "                                        batch_size * n_samples,\n",
    "                                        replacement=True)\n",
    "        \n",
    "        device = \"cuda\" if self.out_embed.weight.is_cuda else \"cpu\"\n",
    "        noise_words = noise_words.to(device)\n",
    "        \n",
    "        ## TODO: get the noise embeddings\n",
    "        noise_vectors = self.out_embed(noise_words)\n",
    "        \n",
    "        # reshape the embeddings so that they have dims (batch_size, n_samples, n_embed)\n",
    "        noise_vectors = noise_vectors.view(batch_size, n_samples, self.n_embed)\n",
    "        \n",
    "        return noise_vectors\n",
    "\n",
    "# Define the loss function for skipgram with negative sampling\n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        \n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "\n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc78114-de9d-4ca7-ae84-71bccd5d10c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [SkipGramNeg] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m NEG \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     log_ps \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(log_ps, targets)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# input, outpt, and noise vectors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:394\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    396\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [SkipGramNeg] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "## Now train the model\n",
    "\n",
    "# check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "NEG = True\n",
    "\n",
    "if NEG == True:\n",
    "    # Get our noise distribution, using word frequencies calculated earlier in the notebook\n",
    "    freqs = Counter(int_words)\n",
    "    word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
    "    unigram_dist = word_freqs/word_freqs.sum()\n",
    "    noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "    embedding_dim = 300\n",
    "    epochs = 5\n",
    "    print_every = 1500\n",
    "    batch_size = 512\n",
    "    model = SkipGramNeg(len(vocab_to_int), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "    criterion = NegativeSamplingLoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "else:\n",
    "    embedding_dim = 300 # you can change, if you want\n",
    "    epochs = 5\n",
    "    print_every = 500\n",
    "    batch_size = 512\n",
    "    model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Now begin training for some number of epochs\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get input and target batches\n",
    "    for inputs, targets in get_batches(train_words, batch_size):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if NEG == True:\n",
    "            # input, outpt, and noise vectors\n",
    "            input_vectors = model.forward_input(inputs) # batch_size x n_embedding\n",
    "            output_vectors = model.forward_output(targets) # batch_size x n_embedding\n",
    "            noise_vectors = model.forward_noise(inputs.shape[0], 5) # batch_size x n_samples x n_embedding\n",
    "            # negative sampling loss\n",
    "            loss = criterion(input_vectors, output_vectors, noise_vectors)           \n",
    "        else:\n",
    "            log_ps = model(inputs)\n",
    "            loss = criterion(log_ps, targets)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % print_every == 0:                  \n",
    "            # getting examples and similarities      \n",
    "            valid_examples, valid_similarities = cosine_similarity(model.in_embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
    "            \n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2094b6-a806-4b49-a4e0-4201d8d6c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "if NEG == True:\n",
    "    model_name_f = 'skipgramNEG_embeddim300_anarchism_wiki.pth'\n",
    "    model_name_w = 'skipgramNEG_embeddim300_anarchism_wiki_weights.pth'\n",
    "else:\n",
    "    model_name_f = 'skipgram_embeddim300_anarchism_wiki.pth'\n",
    "    model_name_w = 'skipgram_embeddim300_anarchism_wiki_weights.pth'\n",
    "\n",
    "torch.save(model.state_dict(), 'models/' + model_name_w)\n",
    "torch.save(model, 'models/' + model_name_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d83e99-3e88-4a80-865a-9ccd1600f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the word vectors\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.in_embed.weight.to('cpu').data.numpy()\n",
    "print(f'Size of embedding {embeddings.shape[0]} x {embeddings.shape[1]}')\n",
    "\n",
    "viz_words = 380\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
