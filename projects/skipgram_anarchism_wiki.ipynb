{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9a1f1e-cf46-4992-8817-0f3696020fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n",
      "Total words in text: 16680599\n",
      "Unique words: 63641\n",
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580]\n",
      "[5233, 10571, 27349, 854, 15067, 58112, 10712, 2731, 371, 539, 2757, 7088, 247, 5233, 44611, 792, 5233, 602, 8983, 4147, 4186, 153, 5233, 447, 1818, 4860, 6753, 7573, 1774, 566]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# read in the extracted text file      \n",
    "with open('../data/text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get list of words\n",
    "words = utils.preprocess(text)\n",
    "print(words[:30])\n",
    "\n",
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words\n",
    "\n",
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "#print(list(word_counts.items())[0:9])  # dictionary of int_words, how many times they appear\n",
    "\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "def discard_prob(threshold, frac):\n",
    "    return 1 - np.sqrt(threshold/frac)\n",
    "\n",
    "train_words = [idx for idx in int_words if random.random() > discard_prob(threshold, word_counts[idx]/len(int_words)) ]\n",
    "print(train_words[:30])\n",
    "\n",
    "# Since the more distant words are usually less related to the current word than those close to it, \n",
    "# we give less weight to the distant words by sampling less from those words in our training examples.\n",
    "# If we choose ùê∂=5, for each training word we will select randomly a number ùëÖ in range  [1:ùê∂], \n",
    "# and then use ùëÖ words from history and ùëÖ words from the future of the current word as correct labels.\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    # implement this function\n",
    "    R = np.ceil(random.random()/0.2).astype(np.int64)\n",
    "    past_idx = list(range(np.max([idx-R, 0]), idx))\n",
    "    future_idx = list(range(idx+1, np.min([idx+R+1, len(words)])))\n",
    "    \n",
    "    return [words[idx] for idx in past_idx + future_idx]\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "\n",
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369f637c-7e68-4c94-a746-158d86954f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Now define this SkipGram model\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        # complete this SkipGram model\n",
    "        self.n_vocabulary = n_vocab\n",
    "        self.n_embedding = n_embed\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_embed, n_vocab)\n",
    "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the forward behavior\n",
    "        x_embed = self.in_embed(x)\n",
    "        x_out = self.fc(x_embed)\n",
    "        x = self.LogSoftmax(x_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define the skipgram model with negative sampling\n",
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        # define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        # Initialize both embedding tables with uniform distribution\n",
    "        self.in_embed.weight.data.uniform_(-1, 1)\n",
    "        self.out_embed.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        # return input vector embeddings\n",
    "\n",
    "        return self.in_embed(input_words)\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        # return output vector embeddings\n",
    "\n",
    "        return self.out_embed(output_words)\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed) \"\"\"\n",
    "        if self.noise_dist is None:\n",
    "            # Sample words uniformly\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # Sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist,\n",
    "                                        batch_size * n_samples,\n",
    "                                        replacement=True)\n",
    "        \n",
    "        device = \"cuda\" if self.out_embed.weight.is_cuda else \"cpu\"\n",
    "        noise_words = noise_words.to(device)\n",
    "        \n",
    "        ## TODO: get the noise embeddings\n",
    "        noise_vectors = self.out_embed(noise_words)\n",
    "        \n",
    "        # reshape the embeddings so that they have dims (batch_size, n_samples, n_embed)\n",
    "        noise_vectors = noise_vectors.view(batch_size, n_samples, self.n_embed)\n",
    "        \n",
    "        return noise_vectors\n",
    "\n",
    "# Define the loss function for skipgram with negative sampling\n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        \n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "\n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc78114-de9d-4ca7-ae84-71bccd5d10c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three | five, of, reputation, anselm, jefferson\n",
      "would | vols, greatly, groundbreaking, participating, conclude\n",
      "at | the, of, energy, on, dynasty\n",
      "a | of, the, and, in, identified\n",
      "th | budget, does, warmer, asserts, mercury\n",
      "he | buna, confronted, its, tobago, representative\n",
      "are | temple, monck, musical, which, the\n",
      "these | ghz, mandan, ross, flat, classification\n",
      "shows | neal, pigeon, belarusians, shaded, civilians\n",
      "operating | powerbook, referee, humor, hugging, checkpoint\n",
      "orthodox | handles, subsidiary, scientists, lost, winfrey\n",
      "applications | ot, benjamin, amman, stabilized, lammas\n",
      "freedom | buffy, xs, pessoa, reciprocated, jointed\n",
      "woman | michael, diphtheria, stressing, technically, divisional\n",
      "quite | showing, protestant, spending, gigs, pnc\n",
      "something | inker, whosoever, mature, lying, mediawiki\n",
      "...\n",
      "into | guthrie, two, seven, the, exercise\n",
      "not | the, a, to, and, in\n",
      "their | of, to, five, the, and\n",
      "who | zero, with, their, production, of\n",
      "often | two, beth, an, s, especially\n",
      "been | this, nine, marshes, as, in\n",
      "some | of, with, a, the, are\n",
      "this | a, and, of, in, to\n",
      "police | weaker, rashi, are, fundamentally, shivaji\n",
      "freedom | including, iowa, commonplace, nicholas, xs\n",
      "troops | summit, revisionist, cyan, carriages, divorced\n",
      "event | empties, happens, voltaic, complicate, pupil\n",
      "san | noon, won, caravaggio, spun, citizenship\n",
      "instance | swedish, franc, relief, fortresses, soiled\n",
      "pre | pictish, aram, recording, idaho, stature\n",
      "construction | cdna, timur, jo, dawn, system\n",
      "...\n",
      "such | be, to, the, this, of\n",
      "only | the, and, for, be, more\n",
      "states | of, and, four, zero, in\n",
      "a | of, the, is, and, as\n",
      "while | in, the, however, and, have\n",
      "called | or, is, the, a, to\n",
      "years | one, the, four, was, and\n",
      "three | one, eight, nine, zero, six\n",
      "engineering | center, tan, used, lsd, wielding\n",
      "older | reappears, granted, degree, color, schwa\n",
      "pope | conjugations, gorham, cortlandt, dbms, equuleus\n",
      "prince | who, that, kicking, rocking, sylhet\n",
      "test | doubly, modern, lessons, collected, constituency\n",
      "units | alienate, excessive, balboa, ephraim, ecuadorian\n",
      "placed | appomattox, beta, guerilla, statistics, qualify\n",
      "hit | monck, athanasius, rito, bambaataa, curling\n",
      "...\n",
      "war | one, the, was, nine, seven\n",
      "called | are, in, a, these, of\n",
      "all | the, of, that, in, which\n",
      "no | jargon, that, in, is, import\n",
      "only | not, to, or, their, is\n",
      "s | the, and, was, a, after\n",
      "nine | eight, one, zero, seven, three\n",
      "an | in, of, the, that, and\n",
      "rise | frequently, liturgy, attention, aedui, compared\n",
      "magazine | beginning, paul, man, necromancer, implications\n",
      "engine | gibbons, engines, can, tear, databases\n",
      "institute | salk, establishment, functionals, fortified, archiepiscopal\n",
      "pope | roman, five, iv, deniers, six\n",
      "discovered | basketball, script, necessary, ch, sgt\n",
      "joseph | his, april, sir, th, had\n",
      "award | awards, american, best, university, ward\n",
      "...\n",
      "s | the, nine, was, one, to\n",
      "american | actor, d, nine, seven, b\n",
      "but | a, were, not, or, it\n",
      "this | to, the, of, as, by\n",
      "nine | one, three, six, seven, eight\n",
      "some | or, of, for, other, as\n",
      "where | with, at, was, a, were\n",
      "were | who, was, to, in, of\n",
      "event | british, their, day, during, who\n",
      "numerous | the, cities, by, world, arab\n",
      "award | awards, received, actor, american, peter\n",
      "san | won, washington, nation, manager, cumberland\n",
      "bible | christians, christianity, catholic, orthodox, church\n",
      "orthodox | jewish, christians, catholic, christian, god\n",
      "stage | much, featuring, bleak, before, was\n",
      "file | data, processor, components, bit, tools\n",
      "...\n",
      "has | as, are, from, and, have\n",
      "had | he, the, his, was, to\n",
      "or | a, are, such, some, certain\n",
      "eight | seven, six, nine, one, five\n",
      "if | we, t, function, y, theorem\n",
      "of | the, and, with, are, by\n",
      "up | to, down, their, a, on\n",
      "their | as, have, and, to, of\n",
      "something | such, what, this, users, capable\n",
      "freedom | social, law, government, authority, liberal\n",
      "proposed | results, applied, between, generated, can\n",
      "san | world, three, city, washington, francisco\n",
      "smith | nine, actor, american, singer, john\n",
      "running | operating, system, design, games, game\n",
      "experience | knowledge, what, being, necessary, nature\n",
      "creation | community, well, school, questions, has\n",
      "...\n",
      "have | and, the, are, to, their\n",
      "if | t, must, function, we, set\n",
      "had | he, was, his, after, the\n",
      "called | as, is, a, the, are\n",
      "at | to, of, the, in, on\n",
      "seven | one, eight, six, two, three\n",
      "and | of, in, the, by, to\n",
      "four | one, zero, two, six, five\n",
      "ice | covered, rock, water, off, beach\n",
      "freedom | society, social, hold, liberal, rights\n",
      "creation | concept, legal, evidence, scientific, information\n",
      "frac | x, function, cdot, sum, y\n",
      "lived | greek, christianity, empire, who, in\n",
      "engineering | technology, fields, institute, systems, electrical\n",
      "joseph | peter, historian, john, david, saint\n",
      "shows | show, tv, television, feature, debut\n",
      "...\n",
      "but | that, the, many, as, are\n",
      "years | zero, age, seven, total, million\n",
      "time | the, to, it, a, play\n",
      "united | states, canada, carolina, south, treaty\n",
      "it | is, to, which, or, with\n",
      "some | are, other, non, these, for\n",
      "all | are, can, this, given, with\n",
      "of | in, the, as, and, with\n",
      "joseph | peter, smith, saint, historian, martin\n",
      "frac | x, cdot, function, sqrt, mathbf\n",
      "liberal | conservative, party, political, parties, economic\n",
      "channel | radio, news, broadcasting, television, bbc\n",
      "file | files, user, interface, software, windows\n",
      "consists | consisting, composed, is, parts, small\n",
      "orthodox | catholic, christianity, christians, churches, jews\n",
      "marriage | married, wife, emperor, divorce, he\n",
      "...\n",
      "an | a, with, for, can, to\n",
      "over | gives, end, finally, from, of\n",
      "where | limit, the, also, allowed, with\n",
      "states | united, economic, union, independent, banking\n",
      "the | as, of, to, a, called\n",
      "s | eight, six, nine, one, zero\n",
      "seven | one, eight, six, four, three\n",
      "which | to, these, is, a, identified\n",
      "smith | peter, jr, eight, williams, actor\n",
      "notes | instrument, isbn, history, press, edition\n",
      "award | awards, received, oscar, winners, academy\n",
      "powers | set, power, independent, formally, branch\n",
      "pre | parts, related, is, old, most\n",
      "paris | de, le, jean, museum, claude\n",
      "egypt | egyptian, arab, cairo, israel, nile\n",
      "file | files, interface, user, formats, data\n",
      "...\n",
      "state | constitution, parliamentary, parliament, of, liberal\n",
      "there | is, only, or, it, any\n",
      "only | a, there, this, with, is\n",
      "are | many, often, some, is, forms\n",
      "where | this, with, known, of, are\n",
      "in | of, the, was, to, and\n",
      "nine | one, five, eight, three, six\n",
      "this | not, that, be, all, it\n",
      "brother | son, king, wife, philip, daughter\n",
      "file | files, software, formats, format, users\n",
      "question | whether, necessary, notion, our, that\n",
      "taking | if, cannot, given, their, seriously\n",
      "stage | album, band, performer, film, featuring\n",
      "ice | water, ocean, surface, wet, lake\n",
      "hold | is, come, them, indeed, not\n",
      "joseph | d, eight, peter, statesman, composer\n",
      "...\n",
      "with | s, and, a, who, in\n",
      "up | the, once, with, them, after\n",
      "three | five, four, zero, seven, one\n",
      "some | these, the, were, so, many\n",
      "had | eventually, was, his, tried, he\n",
      "years | died, husband, birth, age, death\n",
      "use | are, commonly, these, some, other\n",
      "such | used, other, of, well, also\n",
      "applied | individual, or, not, texts, ideas\n",
      "dr | nine, martin, actor, robert, b\n",
      "egypt | egyptian, cairo, ancient, arab, mesopotamia\n",
      "engineering | electrical, technology, computer, research, physics\n",
      "applications | application, software, unix, programming, windows\n",
      "alternative | based, be, classical, various, earlier\n",
      "report | reports, organization, committee, october, commission\n",
      "heavy | metal, low, industry, air, economy\n",
      "...\n",
      "see | links, references, list, in, c\n",
      "a | to, is, as, and, on\n",
      "one | seven, nine, five, eight, three\n",
      "between | of, the, and, constructed, differences\n",
      "has | the, number, including, as, are\n",
      "however | that, these, manner, not, rather\n",
      "over | provided, maximum, total, these, move\n",
      "there | are, have, a, without, tend\n",
      "test | tests, an, nuclear, prevention, effectiveness\n",
      "pressure | gas, force, flow, thermal, reducing\n",
      "account | accounts, about, reports, chapter, money\n",
      "taking | thereby, so, if, to, allowing\n",
      "smith | george, jr, gary, actor, harris\n",
      "except | number, is, exception, it, called\n",
      "bible | testament, biblical, prophets, hebrew, tanakh\n",
      "behind | front, bed, onto, taken, fired\n",
      "...\n",
      "between | the, of, which, and, into\n",
      "two | four, zero, three, one, six\n",
      "while | a, and, such, on, the\n",
      "zero | two, four, five, six, three\n",
      "are | is, there, which, or, as\n",
      "states | united, countries, union, european, republic\n",
      "other | are, with, as, also, which\n",
      "will | when, if, possible, any, prevent\n",
      "universe | infinite, fantasy, sequel, objects, stories\n",
      "applied | which, described, effect, synthesis, form\n",
      "account | according, estimates, billion, debt, note\n",
      "grand | prix, won, duke, cardinal, hockey\n",
      "san | francisco, diego, california, de, cruz\n",
      "creation | itself, ideas, based, criticisms, scientific\n",
      "orthodox | christianity, christians, judaism, testament, catholic\n",
      "placed | tract, head, surrounded, inside, they\n",
      "...\n",
      "th | century, rd, centuries, nd, east\n",
      "american | canadian, actress, association, actor, football\n",
      "people | speak, figures, claim, ethnic, of\n",
      "this | is, to, it, a, of\n",
      "three | two, zero, eight, four, five\n",
      "their | they, have, other, to, the\n",
      "states | united, nation, nations, political, canada\n",
      "had | was, soon, been, led, but\n",
      "issue | copyright, stance, debate, rulings, voters\n",
      "file | files, user, users, windows, formats\n",
      "test | tests, program, testing, methods, expert\n",
      "smith | joseph, stewart, biography, john, simon\n",
      "versions | windows, desktop, version, linux, platform\n",
      "heavy | cavalry, armor, metal, infantry, mounted\n",
      "notes | instrument, reed, alto, bass, diatonic\n",
      "san | francisco, california, santa, diego, cruz\n",
      "...\n",
      "other | of, as, some, or, these\n",
      "d | r, b, l, politician, eight\n",
      "also | the, is, with, word, and\n",
      "s | nine, one, zero, two, five\n",
      "eight | six, five, four, seven, three\n",
      "state | constitution, liberal, counties, independent, union\n",
      "if | we, can, will, an, example\n",
      "into | the, with, several, in, a\n",
      "pre | have, beliefs, emphasis, religion, similarities\n",
      "joseph | robert, william, smith, david, jr\n",
      "running | ran, run, pass, quarterback, corner\n",
      "quite | they, simple, know, necessary, be\n",
      "road | train, routes, route, park, town\n",
      "bill | david, george, jackson, ted, john\n",
      "shown | contain, is, a, certain, for\n",
      "account | estimates, theories, value, to, reports\n",
      "...\n",
      "an | a, the, in, that, which\n",
      "on | the, of, s, to, by\n",
      "state | constitution, independent, republic, democratic, legislative\n",
      "this | but, to, it, a, is\n",
      "war | allied, forces, army, military, led\n",
      "zero | two, five, three, four, six\n",
      "states | united, organization, central, constitution, nations\n",
      "is | a, case, be, can, given\n",
      "mainly | turkish, neighboring, also, ranges, much\n",
      "lived | believed, he, son, had, his\n",
      "creation | development, origins, immortality, beliefs, ideas\n",
      "primarily | farming, mostly, especially, europe, mainly\n",
      "brother | son, daughter, him, his, sons\n",
      "alternative | external, experimental, e, some, complementary\n",
      "derived | form, origin, archaic, suffix, forms\n",
      "instance | common, example, generally, are, is\n",
      "...\n",
      "th | century, rd, nd, one, centuries\n",
      "new | york, one, in, six, the\n",
      "zero | two, four, three, five, one\n",
      "during | war, before, was, after, eight\n",
      "its | of, the, has, central, to\n",
      "often | used, commonly, are, usually, very\n",
      "only | that, same, except, to, when\n",
      "where | the, to, he, at, after\n",
      "pre | of, other, such, recent, are\n",
      "derived | word, uses, aspects, origin, languages\n",
      "animals | animal, species, humans, insects, sheep\n",
      "operating | microsoft, linux, windows, os, desktop\n",
      "universe | cosmic, planet, creator, existence, cosmology\n",
      "assembly | legislative, parliament, deputies, cabinet, government\n",
      "grand | prix, monaco, title, maria, princes\n",
      "primarily | activists, farming, mainly, geography, tend\n",
      "...\n",
      "two | zero, three, four, one, seven\n",
      "eight | six, seven, five, one, nine\n",
      "other | such, are, or, forms, various\n",
      "also | is, of, in, other, and\n",
      "nine | one, eight, five, seven, three\n",
      "these | as, some, are, often, many\n",
      "zero | two, five, three, one, seven\n",
      "war | allied, germany, forces, casualties, troops\n",
      "derived | is, commonly, word, form, called\n",
      "cost | costs, price, output, low, supply\n",
      "pre | early, types, most, with, comes\n",
      "account | billion, estimates, value, debt, price\n",
      "award | awards, nomination, emmy, awarded, academy\n",
      "nobel | prize, laureate, chemist, d, physicist\n",
      "placed | front, chamber, up, small, carved\n",
      "hold | belief, do, judaism, justifications, jew\n",
      "...\n",
      "for | with, a, and, an, as\n",
      "with | and, to, a, as, of\n",
      "he | his, him, was, had, went\n",
      "used | as, or, use, be, common\n",
      "there | which, are, those, between, and\n",
      "one | four, nine, three, six, eight\n",
      "eight | two, one, five, six, seven\n",
      "th | century, history, nd, rd, centuries\n",
      "shown | show, a, using, number, derived\n",
      "alternative | complementary, medicine, mainstream, particular, therapies\n",
      "question | answer, questions, concluded, proposition, whether\n",
      "ocean | atlantic, pacific, islands, coast, arctic\n",
      "placed | inside, surrounded, temple, chamber, carved\n",
      "writers | novelists, literary, poets, literature, authors\n",
      "nobel | prize, laureate, physicist, chemist, recipient\n",
      "discovered | discovery, discoveries, named, found, his\n",
      "...\n",
      "more | than, are, problems, their, tend\n",
      "with | a, which, to, or, are\n",
      "to | a, the, with, but, that\n",
      "often | used, are, sometimes, some, these\n",
      "into | the, which, in, called, this\n",
      "other | are, such, some, as, or\n",
      "he | his, him, who, her, she\n",
      "at | the, in, three, one, four\n",
      "experience | physical, difficulty, phenomena, thoughts, intense\n",
      "road | routes, roads, motorway, lane, railway\n",
      "units | unit, nist, si, armoured, measurement\n",
      "engineering | technology, institute, fields, university, sciences\n",
      "account | according, reports, wrote, reported, death\n",
      "joseph | smith, thomas, apostle, matthew, theologian\n",
      "behind | throwing, began, pulled, strike, lost\n",
      "universe | cosmic, bang, cosmological, galaxies, galaxy\n",
      "...\n",
      "called | the, is, a, this, from\n",
      "from | the, of, in, is, have\n",
      "more | are, than, even, only, difficult\n",
      "american | actress, musician, politician, actor, footballer\n",
      "on | a, the, and, by, first\n",
      "there | are, is, have, or, that\n",
      "with | and, to, the, as, of\n",
      "time | the, an, period, of, and\n",
      "operations | operation, command, unit, security, program\n",
      "articles | org, comprehensive, com, discussion, web\n",
      "units | unit, armoured, charge, speed, si\n",
      "know | we, you, what, sure, so\n",
      "cost | price, costs, market, increase, demand\n",
      "pressure | heat, temperature, velocity, measured, thermal\n",
      "channel | channels, radio, network, cable, broadcasts\n",
      "assembly | legislative, elected, president, elections, minister\n",
      "...\n",
      "see | links, related, also, of, is\n",
      "the | of, in, and, to, a\n",
      "used | other, example, use, also, be\n",
      "s | nine, of, the, by, was\n",
      "can | is, be, if, or, are\n",
      "zero | two, five, four, one, three\n",
      "time | period, have, this, it, present\n",
      "nine | one, eight, five, zero, seven\n",
      "recorded | song, lyrics, songs, band, albums\n",
      "san | francisco, diego, california, los, santa\n",
      "running | operating, ran, run, network, station\n",
      "pre | th, early, sources, century, and\n",
      "numerous | including, most, today, various, notably\n",
      "orthodox | church, ecumenical, orthodoxy, churches, christians\n",
      "joseph | d, william, claude, arthur, physicist\n",
      "frac | sqrt, cdot, qquad, mathrm, infty\n",
      "...\n",
      "was | his, in, the, he, later\n",
      "will | can, do, must, be, cannot\n",
      "d | b, laureate, composer, physicist, three\n",
      "this | of, the, it, in, as\n",
      "see | is, links, references, and, the\n",
      "s | in, the, nine, to, of\n",
      "been | by, were, was, several, that\n",
      "six | seven, five, four, three, eight\n",
      "marriage | married, marry, daughter, marriages, child\n",
      "magazine | published, fiction, book, magazines, journal\n",
      "mean | is, distribution, same, phrase, definition\n",
      "lived | mother, father, parents, wife, moved\n",
      "paris | le, de, france, visited, met\n",
      "award | awards, hugo, winners, awarded, oscar\n",
      "notes | instrument, diatonic, reed, references, chord\n",
      "writers | novelists, poets, authors, writer, living\n",
      "...\n",
      "can | be, if, are, so, require\n",
      "with | the, in, and, of, for\n",
      "american | actress, singer, actor, d, musician\n",
      "would | that, not, to, have, could\n",
      "to | a, that, from, for, it\n",
      "no | not, would, as, have, to\n",
      "has | the, have, in, most, which\n",
      "called | is, a, the, of, are\n",
      "bbc | links, day, external, listing, february\n",
      "construction | constructed, built, textiles, brick, transportation\n",
      "additional | for, construction, required, higher, require\n",
      "frac | cdot, sqrt, equation, mathrm, mbox\n",
      "operations | operation, command, military, base, security\n",
      "file | files, formats, format, user, windows\n",
      "active | passive, radical, chairman, council, member\n",
      "experience | reality, consciousness, experiences, psychological, physical\n",
      "...\n",
      "used | as, or, for, in, use\n",
      "the | of, in, and, from, which\n",
      "but | still, as, the, to, even\n",
      "to | the, when, who, a, it\n",
      "new | york, was, early, in, until\n",
      "th | century, centuries, rd, east, history\n",
      "are | of, these, is, or, other\n",
      "all | they, of, are, which, some\n",
      "hold | to, or, holds, accept, belief\n",
      "consists | branch, council, divided, members, consisting\n",
      "applications | interface, application, software, users, components\n",
      "bill | bills, manager, jerry, miller, tony\n",
      "san | francisco, diego, california, los, angeles\n",
      "issue | issues, stated, critics, rejected, despite\n",
      "frac | sqrt, cdot, mathrm, qquad, equation\n",
      "mathematics | mathematical, mathematicians, geometry, physics, theorem\n",
      "...\n",
      "use | some, others, not, such, these\n",
      "no | as, only, a, the, or\n",
      "however | their, all, for, it, be\n",
      "see | and, other, of, links, references\n",
      "one | four, eight, seven, two, six\n",
      "for | in, a, to, the, and\n",
      "such | or, often, are, used, form\n",
      "these | some, are, of, with, other\n",
      "older | age, females, male, males, birth\n",
      "additional | are, may, must, provide, for\n",
      "institute | technology, university, college, engineering, community\n",
      "pressure | heat, pressures, temperature, gas, flow\n",
      "units | unit, charge, armoured, speed, distance\n",
      "articles | detailed, article, external, online, study\n",
      "ocean | atlantic, pacific, sea, coast, coasts\n",
      "mainly | although, variety, various, parts, inhabiting\n",
      "...\n",
      "new | york, revised, isbn, first, press\n",
      "they | have, to, so, but, not\n",
      "system | systems, an, lower, be, is\n",
      "some | sometimes, are, these, use, have\n",
      "can | be, is, or, if, it\n",
      "five | six, one, four, eight, three\n",
      "called | the, of, this, a, or\n",
      "no | only, there, that, this, is\n",
      "police | murder, suspects, criminal, crime, officers\n",
      "except | may, when, the, is, almost\n",
      "governor | appointed, elected, senate, legislative, chief\n",
      "arts | college, school, university, academy, beaux\n",
      "rise | fall, great, interest, increased, popularity\n",
      "pope | papacy, church, archbishop, deposed, alexandria\n",
      "recorded | band, records, recording, songs, song\n",
      "grand | duchy, princes, prix, knights, won\n",
      "...\n",
      "see | links, external, of, related, list\n",
      "over | the, years, of, one, nine\n",
      "american | actress, actor, canadian, footballer, americans\n",
      "that | be, the, there, this, is\n",
      "as | and, in, the, of, for\n",
      "they | have, their, to, more, so\n",
      "five | four, six, three, zero, eight\n",
      "such | are, use, used, and, some\n",
      "orthodox | church, churches, orthodoxy, ecumenical, patriarch\n",
      "resources | resource, environment, deposits, arable, mineral\n",
      "scale | small, mass, range, magnitude, richter\n",
      "magazine | fiction, published, science, publication, links\n",
      "prince | princess, throne, emperor, empress, crown\n",
      "taking | they, perform, their, take, sent\n",
      "assembly | deputies, legislative, elected, parliamentary, legislature\n",
      "consists | are, comprising, each, located, municipalities\n",
      "...\n",
      "use | used, traditional, commonly, such, are\n",
      "see | links, website, page, related, also\n",
      "american | actor, actress, singer, b, nine\n",
      "may | a, have, case, are, been\n",
      "will | be, shall, do, if, should\n",
      "been | was, have, however, other, although\n",
      "was | he, his, later, in, had\n",
      "it | to, be, is, was, of\n",
      "http | www, htm, org, html, com\n",
      "dr | nine, american, irwin, martin, d\n",
      "except | belonging, only, variety, some, there\n",
      "universe | cosmic, worlds, ultimate, cosmology, marvel\n",
      "bill | clinton, gerald, george, actor, barry\n",
      "square | kilometers, adjacent, kilometres, km, southwest\n",
      "applications | application, software, functionality, developers, clients\n",
      "primarily | other, traditional, mainly, northeastern, classified\n",
      "...\n",
      "were | was, had, and, on, most\n",
      "by | of, in, the, which, was\n",
      "seven | one, three, five, eight, two\n",
      "first | the, s, was, in, zero\n",
      "about | there, good, from, information, com\n",
      "as | in, which, the, many, of\n",
      "which | as, the, to, that, by\n",
      "but | to, that, their, for, when\n",
      "taking | to, a, his, they, them\n",
      "derived | known, derivation, greek, closely, rather\n",
      "test | tests, testing, an, project, at\n",
      "older | families, age, family, household, children\n",
      "pope | pius, cardinal, benedict, archbishop, papal\n",
      "stage | broadway, performances, films, studio, actors\n",
      "troops | army, forces, siege, soldiers, battle\n",
      "question | answer, questions, argument, truth, answered\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "## Now train the model\n",
    "\n",
    "# check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "NEG = True\n",
    "\n",
    "if NEG == True:\n",
    "    # Get our noise distribution, using word frequencies calculated earlier in the notebook\n",
    "    freqs = Counter(int_words)\n",
    "    word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
    "    unigram_dist = word_freqs/word_freqs.sum()\n",
    "    noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "    embedding_dim = 300\n",
    "    epochs = 5\n",
    "    print_every = 1500\n",
    "    batch_size = 512\n",
    "    model = SkipGramNeg(len(vocab_to_int), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "    criterion = NegativeSamplingLoss() \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "else:\n",
    "    embedding_dim = 300 # you can change, if you want\n",
    "    epochs = 5\n",
    "    print_every = 500\n",
    "    batch_size = 512\n",
    "    model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Now begin training for some number of epochs\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get input and target batches\n",
    "    for inputs, targets in get_batches(train_words, batch_size):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        if NEG == True:\n",
    "            # input, outpt, and noise vectors\n",
    "            input_vectors = model.forward_input(inputs) # batch_size x n_embedding\n",
    "            output_vectors = model.forward_output(targets) # batch_size x n_embedding\n",
    "            noise_vectors = model.forward_noise(inputs.shape[0], 5) # batch_size x n_samples x n_embedding\n",
    "            # negative sampling loss\n",
    "            loss = criterion(input_vectors, output_vectors, noise_vectors)           \n",
    "        else:\n",
    "            log_ps = model(inputs)\n",
    "            loss = criterion(log_ps, targets)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % print_every == 0:                  \n",
    "            # getting examples and similarities      \n",
    "            valid_examples, valid_similarities = cosine_similarity(model.in_embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
    "            \n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2094b6-a806-4b49-a4e0-4201d8d6c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "if NEG == True:\n",
    "    model_name_f = 'skipgramNEG_embeddim300_anarchism_wiki.pth'\n",
    "    model_name_w = 'skipgramNEG_embeddim300_anarchism_wiki_weights.pth'\n",
    "else:\n",
    "    model_name_f = 'skipgram_embeddim300_anarchism_wiki.pth'\n",
    "    model_name_w = 'skipgram_embeddim300_anarchism_wiki_weights.pth'\n",
    "\n",
    "torch.save(model.state_dict(), 'models/' + model_name_w)\n",
    "torch.save(model, 'models/' + model_name_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d83e99-3e88-4a80-865a-9ccd1600f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the word vectors\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.in_embed.weight.to('cpu').data.numpy()\n",
    "print(f'Size of embedding {embeddings.shape[0]} x {embeddings.shape[1]}')\n",
    "\n",
    "viz_words = 380\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c7805-18d4-4286-8f26-9d479b9e1799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
