{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45168aaa-5360-4e3c-bf30-80274b62a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re,nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.data.path.append(os.path.join(os.getcwd(), \"nltk_data\"))\n",
    "\n",
    "from nltk.tokenize import textTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('cmudict')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def replace_pattern(pat,str):\n",
    "    # replace all this-pattern strings with str\n",
    "    \n",
    "\n",
    "def print_html_header_paragraph(filepath)\n",
    "    # Open the HTML file and create a BeautifulSoup Object\n",
    "    with open(filepath) as f:\n",
    "        page_content = BeautifulSoup(f, 'lxml')    \n",
    "    # Print only the text from all the <h2> and <p> tags inside the <div> tags that have a class=\"section\" attribute\n",
    "    for section in page_content.find_all('div', class_='section'):\n",
    "        header = section.h2.get_text()\n",
    "        print(header)\n",
    "        paragraph = section.p.get_text()\n",
    "        print(paragraph)\n",
    "\n",
    "def lemmatize_text(all_texts):\n",
    "    # use Porter stemmer\n",
    "    # preprocessing: normalize, remove punctuations and stop words\n",
    "    # input: list of sentences\n",
    "    # output: list of processed sentences\n",
    "    stopwds = stopwords.words('english')\n",
    "    stmr = PorterStemmer()\n",
    "    all_texts_2 = []\n",
    "    for text in all_texts:\n",
    "        txt = text.lower() # convert to lower case\n",
    "        txt = word_tokenize(txt) # tokenize\n",
    "        txt2 = [tx for tx in txt if re.match(r'[^a-zA-Z0-9]', tx) == None ] # remove punctuations \n",
    "        txt2 = [stmr.stem(tx) for tx in txt2 if tx not in stopwds ] # remove stop words and stemming\n",
    "        all_texts_2.append(txt2)\n",
    "    return all_texts_2\n",
    "\n",
    "def stem_text(all_texts, pos='n'):\n",
    "    # use general lemmetization\n",
    "    # preprocessing: normalize, remove punctuations and stop words\n",
    "    # input: \n",
    "    ## all_texts: list of sentences to be processed\n",
    "    ## pos: class of the word to be lemmatized, default 'n'\n",
    "    # output: list of processed sentences\n",
    "    stopwds = stopwords.words('english')\n",
    "    lmtz = WordNetLemmatizer()\n",
    "    all_texts_2 = []\n",
    "    for text in all_texts:\n",
    "        txt = text.lower() # convert to lower case\n",
    "        txt = word_tokenize(txt) # tokenize\n",
    "        txt2 = [tx for tx in txt if re.match(r'[^a-zA-Z0-9]', tx) == None ] # remove punctuations \n",
    "        txt2 = [lmtz.lemmatize(tx, pos) for tx in txt2 if tx not in stopwds ] # remove stop words and stemming\n",
    "        all_texts_2.append(txt2)\n",
    "    return all_texts_2\n",
    "\n",
    "def sentence_count_naive(text):\n",
    "    return len([w for w in text if w == '.'])\n",
    "\n",
    "def sentence_count_regex(text):\n",
    "    return len(re.compile(r'[^\\.]+\\.').finditer(text))\n",
    "\n",
    "def sentence_count_token(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "def word_count_naive(sent):\n",
    "    return len([w for w in sent.replace('.',' ') if w == ' '])\n",
    "\n",
    "def word_count_regex(sent,nonumber=0):\n",
    "    sent2 = sent.replace('.',' ')\n",
    "    sent_compile = re.compile(r'[.+(\\s\\d+)?\\s]+') if nonumber == 1 else re.compile(r'[.+\\s]+')\n",
    "    return len(sent_compile.finditer(sent2))\n",
    "\n",
    "def word_count_token(sent):\n",
    "    return len(word_tokenize(sent))\n",
    "\n",
    "def syllable_count(word):\n",
    "    d = cmudict.dict()\n",
    "    try:\n",
    "        return np.min([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return _syllables(word)\n",
    "\n",
    "def _syllables(word):\n",
    "#referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "def hard_word_count(sent):\n",
    "    # TO DO\n",
    "    return len([w for w in word_tokenize(sent) if syllable_count(WordNetLemmatizer().lemmatize(w, pos='v')) >= 3])\n",
    "\n",
    "# Below are two readability test indices: Fleschâ€“Kincaid Grade and Gunning-Fog Grade\n",
    "def flesch_index(text):\n",
    "    # TO DO\n",
    "    number_of_sentences, sents = sentence_count(text)\n",
    "    number_of_words = np.sum([len(word_tokenize(sent)) for sent in sents])\n",
    "    number_of_syllabi = np.sum([np.sum([syllable_count(w) for w in word_tokenize(sent)])\\\n",
    "                                for sent in sents])\n",
    "    print('There are ' + str(number_of_sentences) + ' sentences in the input text.')\n",
    "    print('There are ' + str(number_of_words) + ' words in the input text.')\n",
    "    print('There are ' + str(number_of_syllabi) + ' syllabi in the input text.')\n",
    "    return 0.39*(number_of_words/number_of_sentences) + \\\n",
    "        11.8*(number_of_syllabi/number_of_words) - 15.59\n",
    "        \n",
    "def fog_index(text):\n",
    "    # TO DO\n",
    "    number_of_sentences, sents = sentence_count(text)\n",
    "    number_of_words = np.sum([len(word_tokenize(sent)) for sent in sents])\n",
    "    number_of_hard_words = np.sum([len([w for w in word_tokenize(sent) if \\\n",
    "            syllable_count(WordNetLemmatizer().lemmatize(w, pos='v')) >= 3]) for sent in sents])\n",
    "    print('There are ' + str(number_of_sentences) + ' sentences in the input text.')\n",
    "    print('There are ' + str(number_of_words) + ' words in the input text.')\n",
    "    print('There are ' + str(number_of_hard_words) + ' hard words in the input text.')\n",
    "    return 0.4*(number_of_words/number_of_sentences) + \\\n",
    "        40*(number_of_hard_words/number_of_words)\n",
    "\n",
    "# clean text to prepare for the conversion to bag-of-words\n",
    "# lowercase, lemmatized w.r.t. nouns and verbs\n",
    "# no digits, no stopwords\n",
    "# stemmized and contains at least three letters\n",
    "def clean_text(txt):\n",
    "    lemm_txt = [ wnl.lemmatize(wnl.lemmatize(w.lower(),'n'),'v') \\\n",
    "                for w in word_tokenizer.tokenize(txt) if \\\n",
    "                w.isalpha() and w not in stop_words ]\n",
    "    return [ sno.stem(w) for w in lemm_txt if w not in stop_words and len(w) > 2 ]\n",
    "\n",
    "# convert a list of words into a \"bag_of_words\" dictionary, including \n",
    "def bag_of_words(words):\n",
    "    bags = dict()\n",
    "    for w in words:\n",
    "        #bags = bags.update({w: bags.get(w)+1}) if w in bags.keys() else bags.setdefault({w: 1})\n",
    "        bags[w] = bags.get(w) + 1 if w in bags.keys() else 1\n",
    "    return bags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a65b69-2173-470e-9e4c-747cdac02e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request and process/parse html text from the webpages\n",
    "# Example 1: Amazon wikipage: https://en.wikipedia.org/wiki/Amazon_(company)\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a Response object\n",
    "r = requests.get('https://en.wikipedia.org/wiki/Amazon_(company)')\n",
    "\n",
    "# Get HTML data\n",
    "html_data = r.text\n",
    "\n",
    "# Create a BeautifulSoup Object\n",
    "page_content = BeautifulSoup(html_data,'html.parser')\n",
    "\n",
    "# Find financial table\n",
    "wikitable = page_content.find('table', {'class': 'wikitable float-left'})\n",
    "\n",
    "# Find all column titles\n",
    "wikicolumns = wikitable.tbody.findAll('tr')[0].findAll('th')\n",
    "\n",
    "# Loop through column titles and store into Python array\n",
    "df_columns = []\n",
    "for tag in wikicolumns:\n",
    "    df_columns.append(tag.get_text(strip=True, separator=\" \").replace('[ 159 ]',''))\n",
    "print(df_columns)\n",
    "\n",
    "# Loop through the data rows and store into Python array\n",
    "df_data = []\n",
    "for row in wikitable.tbody.findAll('tr')[1:]:\n",
    "    row_data = []\n",
    "    for td in row.findAll('td'):\n",
    "        row_data.append(td.get_text(strip=True, separator=\" \"))\n",
    "    df_data.append(np.array(row_data))\n",
    "\n",
    "# Print financial data in DataFrame format and set `Year` as index\n",
    "dataframe = pd.DataFrame(data=df_data, columns=df_columns)\n",
    "dataframe.set_index('Year', inplace=True)\n",
    "dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
